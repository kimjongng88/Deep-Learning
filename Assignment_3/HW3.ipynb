{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'provided modules and data'\n",
    "import wget, os, gzip, pickle, random, re, sys\n",
    "\n",
    "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
    "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
    "\n",
    "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
    "\n",
    "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
    "\n",
    "    cst = 'char' if char else 'word'\n",
    "\n",
    "    imdb_url = IMDB_URL.format(cst)\n",
    "    imdb_file = IMDB_FILE.format(cst)\n",
    "\n",
    "    if not os.path.exists(imdb_file):\n",
    "        wget.download(imdb_url)\n",
    "\n",
    "    with gzip.open(imdb_file) as file:\n",
    "        sequences, labels, i2w, w2i = pickle.load(file)\n",
    "\n",
    "    if voc is not None and voc < len(i2w):\n",
    "        nw_sequences = {}\n",
    "\n",
    "        i2w = i2w[:voc]\n",
    "        w2i = {w: i for i, w in enumerate(i2w)}\n",
    "\n",
    "        mx, unk = voc, w2i['.unk']\n",
    "        for key, seqs in sequences.items():\n",
    "            nw_sequences[key] = []\n",
    "            for seq in seqs:\n",
    "                seq = [s if s < mx else unk for s in seq]\n",
    "                nw_sequences[key].append(seq)\n",
    "\n",
    "        sequences = nw_sequences\n",
    "\n",
    "    if final:\n",
    "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
    "\n",
    "    # Make a validation split\n",
    "    random.seed(seed)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_val, y_val = [], []\n",
    "\n",
    "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
    "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
    "        if i in val_ind:\n",
    "            x_val.append(s)\n",
    "            y_val.append(l)\n",
    "        else:\n",
    "            x_train.append(s)\n",
    "            y_train.append(l)\n",
    "\n",
    "    return (x_train, y_train), \\\n",
    "           (x_val, y_val), \\\n",
    "           (i2w, w2i), 2\n",
    "\n",
    "\n",
    "def gen_sentence(sent, g):\n",
    "\n",
    "    symb = '_[a-z]*'\n",
    "\n",
    "    while True:\n",
    "\n",
    "        match = re.search(symb, sent)\n",
    "        if match is None:\n",
    "            return sent\n",
    "\n",
    "        s = match.span()\n",
    "        sent = sent[:s[0]] + random.choice(g[sent[s[0]:s[1]]]) + sent[s[1]:]\n",
    "\n",
    "def gen_dyck(p):\n",
    "    open = 1\n",
    "    sent = '('\n",
    "    while open > 0:\n",
    "        if random.random() < p:\n",
    "            sent += '('\n",
    "            open += 1\n",
    "        else:\n",
    "            sent += ')'\n",
    "            open -= 1\n",
    "\n",
    "    return sent\n",
    "\n",
    "def gen_ndfa(p):\n",
    "\n",
    "    word = random.choice(['abc!', 'uvw!', 'klm!'])\n",
    "\n",
    "    s = ''\n",
    "    while True:\n",
    "        if random.random() < p:\n",
    "            return 's' + s + 's'\n",
    "        else:\n",
    "            s+= word\n",
    "\n",
    "def load_brackets(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='dyck')\n",
    "\n",
    "def load_ndfa(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='ndfa')\n",
    "\n",
    "def load_toy(n=50_000, char=True, seed=0, name='lang'):\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    if name == 'lang':\n",
    "        sent = '_s'\n",
    "\n",
    "        toy = {\n",
    "            '_s': ['_s _adv', '_np _vp', '_np _vp _prep _np', '_np _vp ( _prep _np )', '_np _vp _con _s' , '_np _vp ( _con _s )'],\n",
    "            '_adv': ['briefly', 'quickly', 'impatiently'],\n",
    "            '_np': ['a _noun', 'the _noun', 'a _adj _noun', 'the _adj _noun'],\n",
    "            '_prep': ['on', 'with', 'to'],\n",
    "            '_con' : ['while', 'but'],\n",
    "            '_noun': ['mouse', 'bunny', 'cat', 'dog', 'man', 'woman', 'person'],\n",
    "            '_vp': ['walked', 'walks', 'ran', 'runs', 'goes', 'went'],\n",
    "            '_adj': ['short', 'quick', 'busy', 'nice', 'gorgeous']\n",
    "        }\n",
    "\n",
    "        sentences = [ gen_sentence(sent, toy) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s : len(s))\n",
    "\n",
    "    elif name == 'dyck':\n",
    "\n",
    "        sentences = [gen_dyck(7./16.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    elif name == 'ndfa':\n",
    "\n",
    "        sentences = [gen_ndfa(1./4.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    else:\n",
    "        raise Exception(name)\n",
    "\n",
    "    tokens = set()\n",
    "    for s in sentences:\n",
    "\n",
    "        if char:\n",
    "            for c in s:\n",
    "                tokens.add(c)\n",
    "        else:\n",
    "            for w in s.split():\n",
    "                tokens.add(w)\n",
    "\n",
    "    i2t = [PAD, START, END, UNK] + list(tokens)\n",
    "    t2i = {t:i for i, t in enumerate(i2t)}\n",
    "\n",
    "    sequences = []\n",
    "    for s in sentences:\n",
    "        if char:\n",
    "            tok = list(s)\n",
    "        else:\n",
    "            tok = s.split()\n",
    "        sequences.append([t2i[t] for t in tok])\n",
    "\n",
    "    return sequences, (i2t, t2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Classification: data loading\n",
    "The IMDb dataset is the sequence-learning equivalent of MNIST: a large, challenging classification dataset with plenty of examples, that is still light enough to train models for on a laptop. It contains 50 000 reviews of movies, taken from the Internet Movie Database which are either highly positive or highly negative. The task is to predict which for a given review. This is known as sentiment analysis. </br>\n",
    "To simplify things we've preprocessed and tokenized the data for you. Our tokenization is a little crude: *we lowercase everything, remove all non-letters and split on whitespace*. It'll do for our current purposes, but in practice, you'd look to more refined tokenization strategies to extract more information from the raw text. *All words have been converted to integer indices in a fixed vocabulary*. \n",
    "\n",
    "To load the data, call the load_imdb function as follows: </br>\n",
    "*(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls =\n",
    "load_imdb(final=False)*\n",
    "\n",
    "If *final* is true, the function returns the canonical test/train split with 25 000 reviews in each.\n",
    "If final is false, a validation split is returned with 20 000 training instances and 5 000\n",
    "validation instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return values are as follows:\n",
    "* x_train: A python list of lists of integers. Each integer represents a word. Sorted from short to long.\n",
    "* y_train: The corresponding class labels: 0 for positive, 1 for negative.\n",
    "* x_val: Test/validation data. Laid out the same as x_train.\n",
    "* y_val: Test/validation labels\n",
    "* i2w: A list of strings mapping the integers in the sequences to their original words. i2w [141] returns the string containing word 141.\n",
    "* w2i: A dictionary mapping the words to their indices. w2i['film'] returns the index for the word \"film\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have a look at your data (always a good idea), \n",
    "# you can convert a sequence from indices\n",
    "# to words as follows\n",
    "print([i2w[w] for w in x_train[141]])\n",
    "print(len(x_train))\n",
    "print(x_train[141])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train, you'll need to loop over x_train and y_train and slice out batches. Each batch will need to be padded to a fixed length and then converted to a torch tensor. Implement this padding and conversion to a tensor.\n",
    "***\n",
    "Tips:\n",
    "* We've included a special padding token in the vocabulary, represented by the string \".pad\". Consult the w2i dictionary to see what the index of this token is.\n",
    "* We've also included special tokens \".start\" and \".end\", which are only used in the autoregressive task.\n",
    "* If you feed a list of lists to the function torch.tensor(), it'll return a torch tensor.\n",
    "    - The inner lists must all have the same size\n",
    "    - Pytorch is pretty good at guessing which datatype (int, float, byte) is expected, but it does sometimes get it wrong. To be sure, add the dataype with *batch = torch.tensor(lists, dtype=torch.long)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.device'>\n"
     ]
    }
   ],
   "source": [
    "'PyTorch modules'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "print(torch.device)\n",
    "\n",
    "'Basic Modules'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings \n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'check what indices are the special tokens'\n",
    "print(w2i['.pad'])\n",
    "print(w2i['.start'])\n",
    "print(w2i['.end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_and_padded_tensors(dt, batch_size, name = 'X'):\n",
    "    'Split data of lists into equal-sized chunks, and pad them'\n",
    "    if name == 'X':\n",
    "        batches = []\n",
    "        for i in range(0, len(dt), batch_size):\n",
    "            if i + batch_size < len(dt):\n",
    "                batches.append(dt[i:i + batch_size])\n",
    "            else:\n",
    "                temp = dt[i:].copy()\n",
    "                # we randomly select samples to fill up this batch\n",
    "    #             temp.extend(random.choices(dt, k=batch_size-len(temp)))\n",
    "                batches.append(temp)\n",
    "        padded_batches = []\n",
    "        for b in batches:\n",
    "#             padded_batches.append(pad_a_batch(b))\n",
    "            padded_batches.extend(pad_a_batch(b))\n",
    "#         padded_batches = torch.tensor(padded_batches, dtype=torch.long)\n",
    "        return padded_batches\n",
    "    else:\n",
    "        return dt\n",
    "#     else:\n",
    "#         # since the label data (y) donot need padding\n",
    "#         padded_batches = torch.tensor(batches, dtype=torch.long)\n",
    "    \n",
    "    # return a list of batch (tensor)\n",
    "#     return padded_batches\n",
    "\n",
    "def pad_a_batch(batch):\n",
    "    \"\"\"\n",
    "    pad 0s to the instances in the batch \n",
    "    (x_train or x_val) to get equal lengths\n",
    "    \"\"\"\n",
    "    max_len = max(len(inst) for inst in batch)\n",
    "    new_batch = []\n",
    "    for ins in batch:\n",
    "        k = ins.copy()\n",
    "        if len(k) < max_len:\n",
    "            k.extend((max_len-len(ins))*[0])\n",
    "            new_batch.append(k)\n",
    "        else:\n",
    "            new_batch.append(k)\n",
    "#     return torch.tensor(new_batch, dtype=torch.long)\n",
    "    return new_batch\n",
    "\n",
    "def preprocess_data(batch_size=5,final_opt = False):\n",
    "    (x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=final_opt)\n",
    "    # split data(lists) into batches(list)\n",
    "    x_train_batch = batched_and_padded_tensors(x_train, batch_size)\n",
    "    y_train_batch = batched_and_padded_tensors(y_train, batch_size, name = 'Y')\n",
    "    x_val_batch = batched_and_padded_tensors(x_val, batch_size)\n",
    "    y_val_batch = batched_and_padded_tensors(y_val, batch_size, name = 'Y')\n",
    "    \n",
    "    return x_train_batch,y_train_batch,x_val_batch,y_val_batch,i2w, w2i, numcls\n",
    "    # generate batch data in form of lists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, i2w, w2i, numcls = preprocess_data(batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train))\n",
    "print(type(x_train[0]))\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: classification, baseline model\n",
    "Question 1: Build a model with the following structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_Net(nn.Module):\n",
    "    # Note: the dim:time here means the instance length in the batched inputs\n",
    "    def __init__(self,numcls=2,token_size=99430,emb=300,hidden=300):\n",
    "        super().__init__()\n",
    "        # torch.nn.Embedding(num_embeddings, embedding_dim) \n",
    "        self.embedding = nn.Embedding(token_size,emb)\n",
    "        self.fc1 = nn.Linear(emb,hidden)\n",
    "        self.fc2 = nn.Linear(hidden,numcls)\n",
    "\n",
    "    def global_max_pool(self,x):\n",
    "        \"\"\"\n",
    "        take the max of an instance \n",
    "        among its time dim -> its words -> the attension\n",
    "        x:(batch,time,hidden)\n",
    "        \"\"\"\n",
    "        (xmax_, xmax_ind) = torch.max(x, 1)\n",
    "        return xmax_\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert torch.is_tensor(x), f'x is not Tensor, x is {type(x)}, {x}'\n",
    "        x = self.embedding(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.global_max_pool(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = simple_Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question2: Train for at least one epoch and compute the validation accuracy. It should be above 60% for most reasonable hyperparameters.\n",
    "* Please remember that training requires implementing a dataloader, a training loop,etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and parameter optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(x_train,y_train, batch_size):\n",
    "#     data = np.array(zip(x_train,y_train))\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(0, len(y_train), batch_size):\n",
    "        if i + batch_size < len(y_train):\n",
    "            x_batches.append(x_train[i:i + batch_size])\n",
    "            y_batches.append(y_train[i:i + batch_size])\n",
    "        else:\n",
    "            x_batches.append(x_train[i:].copy())\n",
    "            y_batches.append(y_train[i:].copy())\n",
    "    training_dt = list(zip(x_batches,y_batches))\n",
    "    random.shuffle(training_dt)\n",
    "    return x_batches,y_batches,training_dt\n",
    "\n",
    "bx_train,by_train,shuffled_tr_dt = dataloader(x_train,y_train, 100)\n",
    "bx_val,by_val,shuffled_val_dt = dataloader(x_val,y_val, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the baseline model, m epochs\n",
    "def easy_train(net,num_epochs, print_int, shuffled_tr_dt, \n",
    "               shuffled_val_dt, criterion, optimizer,PATH = './simple_net.pth'):\n",
    "    # train model\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in tqdm(enumerate(shuffled_tr_dt)):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            # transfer inputs and labels to tensor\n",
    "            inputs_ = torch.tensor(inputs)\n",
    "            labels_ = torch.tensor(labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs_)\n",
    "            loss = criterion(outputs, labels_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % print_int == print_int-1:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_int:.3f}')\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')\n",
    "    \n",
    "    # save model\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "#     model loading\n",
    "#     net = Net()\n",
    "#     net.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    # check validation accuracy\n",
    "    'calculate the accuracy'\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(shuffled_val_dt)):\n",
    "            inputs, labels = data\n",
    "            # transfer inputs and labels to tensor\n",
    "            inputs_ = torch.tensor(inputs)\n",
    "            labels_ = torch.tensor(labels)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(inputs_)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels_.size(0)\n",
    "            correct += (predicted == labels_).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network {100 * correct // total} %')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_train(net,5, 100,shuffled_tr_dt, shuffled_val_dt, criterion, optimizer,PATH = './simple_net.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Writing your own Elman RNN\n",
    "For a sequence-to-sequence layer that propagates information over time, we'll start with a simple Elman network (the first type of simple RNN shown in the lectures).\n",
    "***\n",
    "PyTorch has these available, but it's instructive to build our own first, to see what happens under the hood. Since this can be complicated, we'll give you some skeleton code to work from.\n",
    "***\n",
    "**Question 3**: Fill in the missing parts of this code. And build a second model, like the one in the previous part, but replacing the second layer with an Elman(300, 300, 300) layer.\n",
    "* As you will see when you run the network, looping over slices of your input tensor is horribly slow (remember that every slice becomes a node in the computation graph). This is optimized a lot in the PyTorch library implementations of RNNs, but fundamentally all RNNs require some level of sequential processing.\n",
    "* As we saw in the slides, the first and last hidden layers can be useful for various tricks. This is why PyTorch returns both the output sequence and the last hidden layer.\n",
    "    * Remember this when you fit the Elman module into your larger network: it outputs a pair, and only the first element of that pair should be passed to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman(nn.Module):\n",
    "    def __init__(self,insize=300,outsize=300,hsize=300):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(insize+hsize,hsize)\n",
    "        self.lin2 = nn.Linear(hsize,outsize)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x.shape = (batch, time, embeddings) <dim>\n",
    "        b,t,e = x.size()\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(b,e,dtype=torch.float)\n",
    "            # hidden.shape = (b,e)\n",
    "        outs = []\n",
    "        for i in range(t):\n",
    "            # torch.cat concatenates prev hidden outputs \n",
    "            # to each instance in batch\n",
    "            inp = torch.cat([x[:,i,:],hidden],dim=1)\n",
    "            # inp.shape = (b,e_x+e_h) = (b,2e)\n",
    "            # perform sigmoid to get hidden\n",
    "            hidden = F.sigmoid(self.lin1(inp))\n",
    "            # hidden.shape = (b,e), same as x_t\n",
    "            out = self.lin2(hidden)\n",
    "            # but out is 2D tensor?? how to get None \n",
    "            outs.append(out[:,None,:])\n",
    "        return torch.cat(outs,dim=1), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman_Net(nn.Module):\n",
    "    # Note: the dim:time here means the instance length in the batched inputs\n",
    "    def __init__(self,numcls=2,token_size=99430,emb=300,hidden=300):\n",
    "        super().__init__()\n",
    "        # torch.nn.Embedding(num_embeddings, embedding_dim) \n",
    "        self.embedding = nn.Embedding(token_size,emb)\n",
    "        self.elman = Elman(insize=emb,outsize=hidden)\n",
    "        self.fc2 = nn.Linear(hidden,numcls)\n",
    "\n",
    "    def global_max_pool(self,x):\n",
    "        \"\"\"\n",
    "        take the max of an instance \n",
    "        among its time dim -> its words -> the attension\n",
    "        x:(batch,time,hidden)\n",
    "        \"\"\"\n",
    "        (xmax_, xmax_ind) = torch.max(x, 1)\n",
    "        return xmax_\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert torch.is_tensor(x), f'x is not Tensor, x is {type(x)}, {x}'\n",
    "        x = self.embedding(x)\n",
    "#         x,_ = self.elman(x)\n",
    "        x,_ = self.elman.forward(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.global_max_pool(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# my model is not learning? \n",
    "# because the external forward for Elman should be applied independently? no.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Elman_Net()\n",
    "easy_train(net,2, 25,shuffled_tr_dt, shuffled_val_dt, criterion, optimizer,PATH = './Elman_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Elman_Net()\n",
    "easy_train(net,2, 25,shuffled_tr_dt, shuffled_val_dt, criterion, optimizer,PATH = './Elman_net.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using Torch's RNNs\n",
    "To speed things up, let's try again with pytorch's own RNN implementations. Take the previous model and replace layer 2 with layers of the type *torch.nn.RNN* (the torch implementation of the Elman network) and *torch.nn.LSTM*, and adapt the network as necessary. \n",
    "* Read the documentation carefully to see what the pytorch RNNs compute. The pytorch Elman network only computes the first layer of the Elman network. This means you get the activated hidden layer out.\n",
    "***\n",
    "**Question 4**: Tune the hyperparameters for these three models (MLP, Elman, LSTM).\n",
    "* Machine Learning gospel tells us that with enough tuning, the LSTM will perform best, followed by the Elman network, followed by the MLP. Is this what you find as well? State your hypothesis based on your chosen hyperparameters and the validation performance. Then run all three models on the **test set and report your findings**.\n",
    "* The default dimension order in PyTorch sequence models is slightly confusing (time, batch, embedding). You can transpose the dimensions, or just pass batch_first=True to the constructor of your RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Torch_Elman_Net(nn.Module):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Torch_LSTM_Net(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 4: Autoregressive Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_a_batch_two(batch, max_len):\n",
    "    \"\"\"\n",
    "    pad 0s to the instances in the batch \n",
    "    (x_train or x_val) to get equal lengths\n",
    "    \"\"\"\n",
    "    new_batch = [] # Same as the previous\n",
    "    for ins in batch:\n",
    "        k = ins.copy()\n",
    "        if len(k) < int(max_len):\n",
    "            k.extend((max_len-len(ins))*[0])\n",
    "            new_batch.append(k)\n",
    "        else:\n",
    "            new_batch.append(k)\n",
    "    return new_batch\n",
    "\n",
    "\n",
    "\n",
    "def batched_and_padded_tensors_two(dt, maximum):\n",
    "    'Split data of lists into equal-sized chunks, and pad them'\n",
    "    batches = []\n",
    "    all_batches = []\n",
    "    length = 0\n",
    "    for i in range(len(dt)): # Make sure all is equal\n",
    "        batches.append(dt[i])\n",
    "        length += len(dt[i]) # NOTE we are looking at max tokens instead of sequences\n",
    "        if length < maximum: # If max not reached continue adding sequences\n",
    "            if i+1 == len(dt):\n",
    "                all_batches.appen(batches)\n",
    "                break\n",
    "            continue\n",
    "        else: # If max is reached append\n",
    "            all_batches.append(batches)\n",
    "            length = 0 # Simple reset\n",
    "            batches = []\n",
    "\n",
    "    return all_batches\n",
    "    \n",
    "def finalize_set(x, y):\n",
    "\n",
    "    all_length = []\n",
    "    for a in x: \n",
    "        lengths = max([len(b) for b in a]) # Max token and not sequence. Since all must be equal, easiest is to get the biggest one\n",
    "        all_length.append(lengths) \n",
    "\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    \n",
    "    for i in range(len(x)): # Sequences with unequal length must be adjusted\n",
    "        x_batch.append(pad_a_batch_two(x[i], all_length[i])) # Append batches with equal length\n",
    "        y_batch.append(pad_a_batch_two(y[i], all_length[i])) # Append batches with equal length\n",
    "    \n",
    "    \n",
    "    return x_batch, y_batch\n",
    "\n",
    "\n",
    "def load_in(choice):\n",
    "    if choice == \"ndfa\":\n",
    "        x_train, (i2w, w2i) = load_ndfa(n = 150000)\n",
    "    else:\n",
    "        x_train, (i2w, w2i) = load_brackets(n = 150000)\n",
    "    \n",
    "    print(w2i[\".start\"], w2i[\".end\"]) # Insert 1 at the beginning and 2 at the end to mark start and end of sequence\n",
    "    print(i2w[0])\n",
    "\n",
    "    x = []\n",
    "    for a in x_train:\n",
    "        a.insert(0, 1) # Mark start\n",
    "        a.append(2) # Mark end\n",
    "        x.append(a)\n",
    "    #print(x,\"final\")\n",
    "\n",
    "    y_train = []\n",
    "    for a in x: # Target is just the batch, shifted one token to the left.\n",
    "        b = a[1:] # Remove first column\n",
    "        b.append(0) # Append column of zeros\n",
    "        y_train.append(b)\n",
    "\n",
    "    x_batch = batched_and_padded_tensors_two(x_train, 250) # x_train after batching\n",
    "    y_batch = batched_and_padded_tensors_two(y_train, 250) # y_train after batching\n",
    "\n",
    "    x_final, y_final = finalize_set(x_batch, y_batch) # Set after padding, sequences in batches must have equal length\n",
    "\n",
    "    k = np.array(list(zip(x_final, y_final)))\n",
    "    np.random.shuffle(k)\n",
    "\n",
    "    \n",
    "    return k, (i2w, w2i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as dist\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, insize = 32, num_layer = 2, hsize = 16, vocab = 15):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.insize = insize\n",
    "        self.num_layer = num_layer\n",
    "        self.hsize = hsize\n",
    "        self.embd = nn.Embedding(vocab, insize)\n",
    "        self.Long_Short = nn.LSTM(insize, self.hsize, self.num_layer, batch_first=True)\n",
    "        self.fc = nn.Linear(hsize, self.vocab)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert torch.is_tensor(x), f'x is not Tensor, x is {type(x)}, {x}'\n",
    "        x = self.embd(x)\n",
    "        x,_ = self.Long_Short(x)\n",
    "        x = x.reshape(x.shape[0] * x.shape[1], self.hsize)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(lnprobs, temperature):\n",
    "    \"\"\"\n",
    "    Sample an element from a categorical distribution\n",
    "    :param lnprobs: Outcome logits\n",
    "    :param temperature: Sampling temperature. 1.0 follows the given distribution, 0.0 returns the maximum probability element.\n",
    "    :return: The index of the sampled element.\n",
    "    \"\"\"\n",
    "    if temperature == 0.0:\n",
    "        return lnprobs.argmax()\n",
    "    p = F.softmax(lnprobs / temperature, dim=0)\n",
    "    cd = dist.Categorical(p)\n",
    "    return cd.sample()\n",
    "\n",
    "def pred(net, data, seq, temp, max_length):\n",
    "    print(\"pred\")\n",
    "    predicted_values = []\n",
    "    for i in range(0, max_length):\n",
    "        a = torch.tensor([data.w2i[i] for w in seq[i:]])\n",
    "        b = net.forward(a)\n",
    "        final_position = b[0][-1]\n",
    "        sample_found = sample(final_position, temp)\n",
    "        sample_print = seq.data.i2w[sample_found]\n",
    "        predicted_values.append(sample_print)\n",
    "        if sample_print == \".end\": # If .end is found before the maximum length, return the sample\n",
    "            return pred\n",
    "    return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_train_two(net, print_int, data, criterion,  optimizer, epoch, seq, PATH):\n",
    "    # train model\n",
    "    loss_full = []\n",
    "    for epoch in range(epoch):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "        for batch in data:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = batch[0], batch[1]\n",
    "            # transfer inputs and labels to tensor\n",
    "            inputs_ = torch.tensor(inputs)\n",
    "            labels_ = torch.tensor(labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            labels_ = labels_.reshape(labels_.shape[0]*labels_.shape[1],)\n",
    "            # forward + backward + optimize\n",
    "            outputs= net(inputs_)\n",
    "            loss = criterion(outputs, labels_)\n",
    "            loss.backward()\n",
    "            loss_full.append(loss.item())\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % print_int == print_int-1:    \n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_int:.3f}')\n",
    "                running_loss = 0.0\n",
    "            i += 1\n",
    "        \n",
    "        for b in range(10):\n",
    "            print(b)\n",
    "            predict = pred(net, data, seq, 0.1, 20)\n",
    "            print(predict)\n",
    "            \n",
    "    print('Finished Training')\n",
    "    \n",
    "    # save model\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "#     model loading\n",
    "#     net = Net()\n",
    "#     net.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    # check validation accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    norm = []\n",
    "    # No gradient calculation\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        for par in net.parameters():\n",
    "            total += par.grad.data.pow(2).sum()\n",
    "        total = total.sqrt().item()\n",
    "        norm.append(total)\n",
    "        print(norm)\n",
    "    '''\n",
    "    print(np.mean(loss_full))\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      ".pad\n",
      "[1,  1000] loss: 2.292\n",
      "[1,  2000] loss: 0.967\n",
      "[1,  3000] loss: 0.341\n",
      "[1,  4000] loss: 0.253\n",
      "[1,  5000] loss: 0.230\n",
      "[1,  6000] loss: 0.224\n",
      "[1,  7000] loss: 0.213\n",
      "[1,  8000] loss: 0.211\n",
      "[1,  9000] loss: 0.206\n",
      "0\n",
      "pred\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'w2i'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5r/b_6ghrq14b3djqn0dmwjjq000000gn/T/ipykernel_1124/3033441951.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'.start'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0measy_train_two\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./Elman_net.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/5r/b_6ghrq14b3djqn0dmwjjq000000gn/T/ipykernel_1124/3857311511.py\u001b[0m in \u001b[0;36measy_train_two\u001b[0;34m(net, print_int, data, criterion, optimizer, epoch, seq, PATH)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5r/b_6ghrq14b3djqn0dmwjjq000000gn/T/ipykernel_1124/1378117458.py\u001b[0m in \u001b[0;36mpred\u001b[0;34m(net, data, seq, temp, max_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredicted_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mfinal_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5r/b_6ghrq14b3djqn0dmwjjq000000gn/T/ipykernel_1124/1378117458.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredicted_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mfinal_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'w2i'"
     ]
    }
   ],
   "source": [
    "data,(i2w, w2i) = load_in(\"ndfa\")\n",
    "net = LSTM()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 1\n",
    "seq = ['.start', 'a', 'b']\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
    "easy_train_two(net, 1000, data, criterion,  optimizer, epoch, seq, PATH = './Elman_net.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "name": "python387jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}